{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-LDA - Alanine dipeptide\n",
    "\n",
    "Tutorial for the training of the Deep-LDA collective variable, using the Alanine Dipeptide system as example with the *interatomic distances* as input descriptors.\n",
    "\n",
    "Reference: Bonati, Rizzi and Parrinello, J. Phys. Chem. Lett., 11, 2998-3004 (2020).\n",
    "\n",
    "This page is dapted from [MLCVS docs](https://mlcvs.readthedocs.io/en/latest/notebooks/ala2_deeplda.html)\n",
    "\n",
    "Download data: `https://github.com/luigibonati/mlcvs/tree/main/docs/notebooks/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "dir_nb = Path(globals()['_dh'][0])\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dir_base = dir_nb/'deep_LDA'\n",
    "# if not dir_base.is_dir(): dir_base.mkdir()\n",
    "# os.chdir(dir_base)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ramachandran(x,y,z,scatter=None,ax=None):\n",
    "    # Setup plot\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(5,4.), dpi=100)\n",
    "        ax.set_title(f'Ramachandran plot')\n",
    "\n",
    "    # Plot countour plot\n",
    "    h = ax.hexbin(x,y,C=z,cmap='fessa')\n",
    "    cbar = plt.colorbar(h,ax=ax)\n",
    "    cbar.set_label(f'Deep-LDA CV')\n",
    "\n",
    "    axs[0].set_xlabel(r'$\\phi$ [rad]')\n",
    "    axs[0].set_ylabel(r'$\\psi$ [rad]')\n",
    "\n",
    "def plot_cv_histogram(s,label=None,ax=None,**kwargs):\n",
    "    # Setup plot\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(5,4.), dpi=100)\n",
    "        ax.set_title('Histogram')\n",
    "\n",
    "    if (type(s)==torch.Tensor):\n",
    "        s = s.squeeze(1).detach().numpy()\n",
    "\n",
    "    # Plot histogram\n",
    "    ax.hist(s,**kwargs)\n",
    "    if label is not None:\n",
    "        ax.set_xlabel(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Load data\n",
    "\n",
    "Load the descriptors from PLUMED COLVAR files (one unbiased run for every metastable state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlcvs.utils.io import load_dataframe\n",
    "\n",
    "filenames = [ \"data/ala2_md/COLVAR_stateA\", \"data/ala2_md/COLVAR_stateB\" ]\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for i,file in enumerate(filenames):\n",
    "    data = load_dataframe(file)[::4]\n",
    "\n",
    "    # Descriptors\n",
    "    selection = 'd'\n",
    "    X.append( data.filter(regex=selection).values )\n",
    "    names = data.filter(regex=selection).columns.values\n",
    "\n",
    "    # Labels\n",
    "    y.append( np.full(len(data),i) )\n",
    "\n",
    "X = torch.Tensor( np.vstack(X) )\n",
    "y = torch.Tensor( np.hstack(y) )\n",
    "\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset, split in train and validation, and initialize a `FastTensorDataLoaderâ€™ for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,random_split\n",
    "from mlcvs.utils.data import FastTensorDataLoader\n",
    "\n",
    "dataset = TensorDataset(X,y)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "\n",
    "train_data, valid_data = random_split(dataset,[train_size,valid_size])\n",
    "train_loader = FastTensorDataLoader(train_data)\n",
    "valid_loader = FastTensorDataLoader(valid_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CV\n",
    "\n",
    "Inizialize the neural network and the optimizer and define when to stop the training (EarlyStopping or after a given number of epochs).\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :- | :- | :- |\n",
    "| **Neural network** |\n",
    "| nodes | list | NN architecture (last value equal to the number of hidden layers which are input of LDA) |\n",
    "| activ_type | string | Activation function (relu,tanh,elu,linear) |\n",
    "| n_eig | int | Number of eigenvalues to optimize (or if loss_type=single which one to select) |\n",
    "| **Optimization** |\n",
    "| lrate | float | Learning rate |\n",
    "| sw_reg | float | S_w matrix regularization | \n",
    "| l2_reg | float | L2 regularization |\n",
    "| num_epochs | int | Number of epochs |\n",
    "| **Early Stopping** |\n",
    "| earlystop | bool | Whether to use early stopping based on validation loss |\n",
    "| es_patience | int | Number of epochs before stopping |\n",
    "| es_consecutive | bool | Whether es_patience should count consecutive (True) or cumulative patience |\n",
    "| es_min_delta | float | Minimum decrease of validation loss |\n",
    "| **Log** |\n",
    "| log_every | int | How often print the train/valid loss during training |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Normalization: the following features have a range of values < 1e-6: tensor([[ 0],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [24],\n",
      "        [30],\n",
      "        [31],\n",
      "        [39],\n",
      "        [40],\n",
      "        [44]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tha/app/miniconda3/envs/py39mlcvs/lib/python3.9/site-packages/mlcvs/lda/lda.py:118: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n",
      "L = torch.cholesky(A)\n",
      "should be replaced with\n",
      "L = torch.linalg.cholesky(A)\n",
      "and\n",
      "U = torch.cholesky(A, upper=True)\n",
      "should be replaced with\n",
      "U = torch.linalg.cholesky(A).mH().\n",
      "This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525495809/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:1615.)\n",
      "  L = torch.cholesky(S_w, upper=False)\n",
      "/home/tha/app/miniconda3/envs/py39mlcvs/lib/python3.9/site-packages/mlcvs/lda/lda.py:127: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.\n",
      "The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n",
      "L, _ = torch.symeig(A, upper=upper)\n",
      "should be replaced with\n",
      "L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n",
      "and\n",
      "L, V = torch.symeig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at /opt/conda/conda-bld/pytorch_1670525495809/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2794.)\n",
      "  eigvals, eigvecs = torch.symeig(S_new, eigenvectors=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch           loss_train      loss_valid      \n",
      "100             -40.439 -40.343 \n",
      "200             -51.941 -52.373 \n",
      "300             -62.380 -62.299 \n",
      "INFO: Early stopping\n",
      "396             -62.579 -62.494 \n"
     ]
    }
   ],
   "source": [
    "from mlcvs.lda import DeepLDA_CV\n",
    "\n",
    "#------------- PARAMETERS -------------\n",
    "nodes             = [n_features,30,30,5]\n",
    "\n",
    "lrate             = 0.001\n",
    "sw_reg            = 0.05\n",
    "l2_reg            = 1e-5\n",
    "\n",
    "num_epochs        = 1000\n",
    "earlystop         = True\n",
    "es_patience       = 20\n",
    "es_consecutive    = True\n",
    "es_min_delta      = 0.02\n",
    "\n",
    "log_every         = 100\n",
    "#--------------------------------------\n",
    "\n",
    "# DEVICE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# MODEL\n",
    "model = DeepLDA_CV(nodes)\n",
    "model.set_device(device)\n",
    "\n",
    "# OPTIMIZER\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lrate, weight_decay=l2_reg)\n",
    "model.set_optimizer(opt)\n",
    "\n",
    "# REGULARIZATION\n",
    "model.set_regularization(sw_reg=sw_reg)\n",
    "model.set_earlystopping(patience=es_patience,consecutive=es_consecutive,min_delta=es_min_delta)\n",
    "\n",
    "# TRAIN\n",
    "model.fit(train_loader, valid_loader, standardize_inputs = True, log_every=log_every)\n",
    "\n",
    "# standardize outputs\n",
    "#model.standardize_outputs(train_data[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLDA_CV(\n",
       "  (nn): Sequential(\n",
       "    (0): Linear(in_features=45, out_features=30, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=30, out_features=30, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=30, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get back to CPU\n",
    "model.to('cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39mlcvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46d932b4bae93e09e8426bdbcbc4ae848ccc3c92e33679711ceb9128bb24acb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
